# fixed params
sample_rate: 24000
text_encoder_input_size: 512
llm_input_size: 1024
llm_output_size: 1024

# target: vq
target: lm

data:
    training_files_gpt: 'datasets/all_data.h5'
    max_wav_value: 32768.0
    sampling_rate: 24000
    filter_length: 1024
    hop_length: 240
    win_length: 1024
    n_mel_channels: 128
    mel_fmin: 0.0
    mel_fmax: null
    unit_interpolate_mode: 'nearest'
    text_vocab_size: 128258

train:
    ssl: true
    val_freq: 100
    save_freq: 1000
    seed: 1234
    train_steps: 10000000
    logs_folder: 'logs'
    learning_rate: 0.0001
    betas: [ 0.8, 0.99 ]
    eps: 1.0e-09
    batch_size: 4
    gradient_accumulate_every: 4
    fp16_run: false
    half_type: fp16
    lr_decay: 0.999996
    segment_size: 10080
    init_lr_ratio: 1
    warmup_epochs: 0
    c_mel: 45
    c_kl: 1.0
    num_workers: 8
    keep_ckpts: 15
    delay: [0,2,2,2,2,2,2,2]

t2s:
    dim: 1024
    n_layers: 10
    n_heads: 8
    n_kv_heads: null
    vocab_size: 0
    multiple_of: 256  # make SwiGLU hidden layer size multiple of large power of 2
    ffn_dim_multiplier: null
    norm_eps: 1.0e-5
    rope_theta: 500000

    max_batch_size:  32
    max_seq_len: 2048


s2a:
    dim: 1024
    n_layers: 6
    n_heads: 8
    n_kv_heads: null
    vocab_size: 0
    multiple_of: 256  # make SwiGLU hidden layer size multiple of large power of 2
    ffn_dim_multiplier: null
    norm_eps: 1.0e-5
    rope_theta: 500000

    max_batch_size:  32
    max_seq_len: 2048
vq:
    sample_rate: 24000
    encoder_dim: 64
    encoder_rates: [3, 5, 8, 8]
    decoder_dim: 1536
    decoder_rates: [8, 8, 5, 3]
    # Quantization
    n_codebooks: 8
    codebook_size: 2048
    codebook_dim: 8
    quantizer_dropout: 0.25

# Discriminator
discriminator:
    sample_rate: 24000
    rates: []
    periods: [2, 3, 5, 7, 11]
    fft_sizes: [2048, 1024, 512]
    bands:
      - [0.0, 0.1]
      - [0.1, 0.25]
      - [0.25, 0.5]
      - [0.5, 0.75]
      - [0.75, 1.0]

lambdas:
  mel/loss: 15.0
  adv/feat_loss: 2.0
  adv/gen_loss: 1.0
  vq/commitment_loss: 0.25
  vq/codebook_loss: 1.0
  semantic/loss: 100.0

# Loss setup
MultiScaleSTFTLoss:
    window_lengths: [2048, 512]
MelSpectrogramLoss:
    n_mels: [5, 10, 20, 40, 80, 160, 320]
    window_lengths: [32, 64, 128, 256, 512, 1024, 2048]
    mel_fmin: [0, 0, 0, 0, 0, 0, 0]
    mel_fmax: [null, null, null, null, null, null, null]
    pow: 1.0
    clamp_eps: 1.0e-5
    mag_weight: 0.0
